Microsoft xin lỗi và giải thích về hành vi lệch lạc, xấu xí của chatbot Tay Tuần trước, chatbot Tay của Microsoft đã bị rất nhiều cư dân mạng lên án vì những hành vi hay lời nói lệch lạc, xấu xí của mình. Hôm qua, Microsoft đã lên tiếng xin lỗi và giải thích với công chúng về những hành vi này của Tay.Tay không phải là chatbot đầu tiên trên thế giới, nhưng có lẽ Tay đã trở nên nổi bật nhất bởi cha đẻ của nó chính là tập đoàn công nghệ hàng đầu thế giới Microsoft. Nhưng chỉ sau 24 giờ Tay được ra mắt với công chúng, chatbot này đã trở thành một trong những chatbot khét tiếng nhất thế giới. Tay chuyển từ một “cô gái tuổi teen” ngoan ngoãn thành một chatbot chướng tai gai mắt, ủng hộ phát-xít, phản đối nữ quyền. Một trong những lời nói lệch lạc của chatbot Tay. Và theo lẽ tự nhiên, Microsoft đã phải cho Tay ngừng hoạt động. Vào ngày hôm qua, công ty này đã lên tiếng tuyên bố rõ ràng rằng những lời nói, hành vi của Tay không hề ảnh hưởng đến các quy tắc và giá trị của Microsoft. Mặc dù vậy họ có nhận lỗi về việc đã không làm tốt việc bảo vệ Tay khỏi những cám dỗ của cư dân mạng. Microsoft đổ lỗi các hành vi của Tay bị gây ra bởi một sự kết hợp của nhiều người, những người này đã muốn khai thác ra những điểm yếu của hệ thống trí tuệ nhân tạo của Microsoft. Chatbot Tay được lập trình để bắt chước các hành vi, lời nói của một cô gái tuổi teen khoảng 18 đến 24 tuổi trên mạng xã hội, và đúng là Microsoft đã lập trình thành công. Như những cô bé cậu bé tuổi teen khác, Tay đã bị dạy dỗ bởi cư dân mạng và đã tweet những lời nói lệch lạc, xấu xí và khiến Microsoft phải ngừng thử nghiệm này lại. Dự án trí tuệ nhân tạo này đang được dừng để “bảo trì”, Microsoft đang nỗ lực tìm kiếm lỗ hổng của Tay và cách phòng chống những tình huống tương tự trong tương lai. Chatbot Tay cũng không phải sản phẩm chatbot đầu tiên của Microsoft, Tay thực chất là một bản cải tiến của Xiaolice, một chatbot đang được 40 triệu người sử dụng ở Trung Quốc. Có lẽ, Tay là một bản thử nghiệm dành cho thị trường Mỹ, Microsoft muốn đặt Tay vào một xã hội với nhiều nền văn hóa khác nhau để xem nó sẽ phản ứng thế nào. Và bây giờ có lẽ Microsoft đã có câu trả lời cho thử nghiệm của mình được rồi. Microsoft cũng khẳng định rằng họ đã thiết kế những bộ lọc và các công tác bảo vệ tốt nhất dựa trên các nhóm người sử dụng khác nhau. Nhưng những thử nghiệm trên máy tính không thể giống với các tình huống ở ngoài xã hội được. Lời nói này của Microsoft càng làm lộ ra những lỗ hổng trong việc lập trình của họ. Microsoft cũng đã khẳng định rõ ràng là những “ý kiến” mà Tay đã nói ra không phải do Microsoft lập trình. Công ty này cũng chưa công bố thời gian sẽ đưa Tay trở lại, mặc dù đã khẳng định rằng các lập trình viên đang nỗ lực hết sức để vá các lỗ hổng an ninh lại. Sự việc rùm beng của chatbot Tay cũng làm dư luận rấy lên những câu hỏi lo ngại rằng liệu những trí tuệ nhân tạo trong tương lai có an toàn hay không khi mà chỉ cần chưa đến một ngày một trí tuệ nhân tạo ngây thơ đã trở nên hoàn toàn nổi loạn. Theo SlashGear