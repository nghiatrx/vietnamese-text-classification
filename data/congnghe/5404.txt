Hiểu được ảnh là "chuyện nhỏ", bước tiếp theo của AI là hiểu được video Việc nhận biết được các hoạt động trong thời gian thực có thể sẽ là bước tiến lớn trong công cuộc hỗ trợ AI nhận thức được về thế giới bên ngoài.Hiện tại, nếu một máy tính có khả năng nhận ra một con mèo hay con vịt trong một hình ảnh tĩnh, thì máy tính đó đã khá là thông minh. Tuy nhiên, trong tương lai, AI sẽ phải có khả năng nhận ra được các hành động của con mèo, như việc đuổi bắt con vịt quanh bếp chẳng hạn. MIT và IBM đã cho ra một tập hợp dữ liệu khổng lồ bao gồm các video clip được chú thích chi tiết về các hành động được thực hiện trong đó. Các dữ liệu trong bộ số liệu thời gian bao gồm các thước phim ba giây về nhiều hoạt động khác nhau, như câu cá, hay nhảy tự do. Chỉ trong thước phim 3 giấy, AI đã liên tục đưa ra các dự đoán về hành động: tập thể dục - giữ thăng bằng - co giãn cơ thể. Aude Oliva, một nhà nghiên cứu khoa học tại MIT và là một trong những người đứng đằng sau dự án cho biết: " Sự bùng nố trong công nghệ trí tuệ nhân tạo đã được châm ngòi nhờ vào thành công trong việc dạy cho máy tính nhận biết được nội dung của các hình ảnh tĩnh bằng cách huấn luyện các mạng thần kinh sâu trong các bộ dữ liệu mang nhãn lớn. Các hệ thống AI mà có khả năng phân tích video hiện tại, bao gồm cả các hệ thống trong xe tự lái, thường dựa vào việc định dạng các vật thể trong các khung hình tĩnh thay vì phân tích các hành động. Mới đây Google đã tung ra một công cụ có khả năng nhận dạng các vật thể trong video, một trong những tính năng của Cloud Platform, một dịch vụ đã bao gồm các công cụ AI để xử lý ảnh, âm thanh và văn bản. Thách thức kế tiếp sẽ ngoài việc dạy cho máy móc hiểu được video chứa những vật thể gì, chúng ta còn phải dạy cho chúng biết rằng điều gì đang diễn ra trong những thước phim đó. Ứng dụng này có thể đem lại nhiều lợi ích thiết thực, chẳng hạn như tạo ra những phương pháp mới để tìm kiếm, chú thích và khai thác các đoạn video. Ứng dụng này cũng sẽ giúp cho các con robot hoặc các xe tự lái có khả năng hiểu được thế giới xung quanh tốt hơn. Dự án MIT-IBM thực tế chỉ là một trong nhiều các bộ dữ liệu video được thiết kế để thúc đẩy tiến bộ trong việc đào tạo máy móc để hiểu được các hành động trong thế giới bên ngoài. Ví dụ, vào năm ngoái, Google đã phát hành một bộ gồm 8 triệu video YouTube gọi là YouTube-8M. Facebook cũng đang phát triển một bộ dữ liệu bao gồm các thước phim, được chia làm các bộ Cảnh, Hành động và Hiện vật. Olga Russakovsky, trợ lý giáo sư tại Đại học Princeton, chuyên về viễn thám máy tính, cho biết họ đang gặp khó khăn trong việc phát triển các bộ dữ liệu video hữu ích vì chúng đòi hỏi nhiều dung lượng lưu trữ và tính toán hơn so với các bức ảnh tĩnh. Các nhà khoa học khác cũng đang áp dụng những cách tiếp cận sáng tạo với vấn đề này. Twenty Billion Neurons, một công ty mới thành lập ở Toronto và Berlin, đã tạo ra một bộ dữ liệu tuỳ chỉnh bằng cách trả lương cho nhân viên để thực hiện các tác vụ đơn giản. Một trong những nhà sáng lập của công ty, ông Roland Memisevic, nói rằng công ty cũng sử dụng mạng thần kinh được thiết kế đặc biệt để xử lý thông tin thị giác theo thời gian. Ông nói: Danny Gutfreund, nhà nghiên cứu tại IBM, người cộng tác trong dự án, cho biết việc nhận biết các hành động hiệu quả sẽ đòi hỏi máy tính phải học về việc một con người đang thực hiện một hành động, và sau đó chuyển giao kiến thức này vào một trường hợp khác, khi mà một con vật cũng đang thực hiện hành động tương tự. Tiến bộ trong lĩnh vực này, gọi là học tập chuyển tiếp, sẽ trở nên quan trọng cho tương lai của AI. Gutfreund cho biết thêm rằng công nghệ cũng có những ứng dụng thiết thực. Tham khảo Technology Review